Knowledge points:
深度学习模型步骤： 模型定义->模型训练->模型预测
一、损失函数
    squared_loss :找出一组参数，使训练样本平均损失最小
二、优化算法
    1、解析解：例如线性回归，loss可以用公式表达出来；
    2、数值解：例如softmax回归，loss不能直接用公式表示出来，只能通过对初始的参数进行不断迭代来减小loss
       a.SGD算法：（mini-batch stochastic gradient descent）：小批量随机梯度下降法：先选取一组模型参数的初始值，如随机选取。然后通过数次的迭代使该模型参数的loss不断减小。在每次迭代中，在samples中随机均匀选取batch个特征，计算batch_loss的mean（），再求mean（）关于模型参数的grad，
    并且用得到的grad乘以超参数学习率（learning—rate>0）来作为此次迭代模型参数的减小量。
    w1 <- w1 - grad * lr
    ...
         (1)d2l.sgd():
             def sgd(params, lr, batch_size):
                 for param in params:
                     param.data -=  lr * param.grad / batch_size
         （2）torch.optim.SGD(net.parameters(), lr):
               没有除以batch_size     
三、利用nn.Sequential()搭建net：
   #写法一：
      net = nn.Sequential(nn.Linear(num_inputs, 1))
   #写法二：
      net = nn.Sequential()
      net.add_module('linear', nn.Linear(num_inputs, 1))
   #写法三：
      from collections import OrderedDict
      net = nn.Sequential(OrderedDict([('linear',nn.Linear(num_inputs, 1))]))
 四、利用torch.nn.init初始化模型参数
     import torch.nn as nn
     nn.init.normal_(param, mean, std)
     nn.init.constant_(param, mean, std)
 五、均方损失函数（torch.nn.MSELoss（））
     一般的损失函数直接计算batch的数据，即直接返回维度为（batch， 1）的loss向量， 且一般的损失函数都有size——average和reduce两个参数，
 若reduce=False，则直接返回向量形式的loss，反之则返回标量mean；若aver=True， 返回mean（）， 反之返回sum（）， 默认情况下MSELoss（）
 函数的reduce=True， size——average=True。
 六、调整学习率
     for param_group in optimzier.param_groups:
         param_group['lr'] = 0.3
 

    
